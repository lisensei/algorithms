import torch
import torch.nn as nn
import torch.utils
import torch.utils.data as Data
import torch.nn.utils
import torchvision.transforms as transform
import torchvision.datasets
import torchvision.models
import sklearn.datasets as datasets
import matplotlib.pyplot as plt
import math
import time
#TODO use argparse for passing hyper parameters (later we will use console based HPC instead of Google Colab)
#TODO use cuda for model training


device= torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
data=torchvision.datasets.MNIST(root='/data',transform=transform.ToTensor(),download=True)

percent=0.01;
batchSize=16;
sampleAmount=40000
noisyAmount=10000
X=data.data[:sampleAmount]
Y=data.targets[:sampleAmount]
noisyX=data.data[sampleAmount:sampleAmount+noisyAmount]
noisyY=data.targets[sampleAmount:sampleAmount+noisyAmount]
testX=data.data[sampleAmount+noisyAmount:]
testY=data.targets[sampleAmount+noisyAmount:]

X=X.to(torch.float32).unsqueeze(dim=1)
testX=testX.to(torch.float32).unsqueeze(dim=1)

def addNoise(source):

    for image in source:
        for j in range(51):
            x = torch.randint(0, 28, (1,))
            y = torch.randint(0, 28, (1,))
            if (x + y) % 2 == 0:
                image[x, y] = 55
            else:
                image[x, y] = 255


    return source


def addGaussianNoise(source,std):
    for image in source:
        image=image+torch.randn(image.size())*std/255
    return source
# X[0] (B, 28, 28)
# X[0] (B, 1, 28, 28)

noisyX=addGaussianNoise(noisyX,51)




# D
# int(len(trainX) * 0.8)
trainSetX=X
trainSetY=Y
trainSet=Data.TensorDataset(trainSetX,trainSetY)
testSet=Data.TensorDataset(testX,testY)
trainSet=Data.DataLoader(dataset=trainSet,batch_size=batchSize,shuffle=True)
testSet=Data.DataLoader(dataset=testSet,batch_size=batchSize,shuffle=False)





#TODO Research part:
# 1. Add noise 20% of image with gaussian (20% of image as value 0.0) to 40% of dataset
# 2. Try to train whole dataset at once
# 3. Try to train clean dataset first and then after 10 epochs add 20% of noisy dataset

#TODO implement dataset via for train and test part
# torch.utils.data.Dataset
# torch.utils.data.DataLoader



#TODO test out different pre-tained models
class Network(nn.Module):
    def __init__(self,inputFeatures,numberOfClasses):
        super(Network, self).__init__()

        # loaded pretrained model
        self.pretrained_model = torchvision.models.resnet18(pretrained=True)

        # modify first layer to fit the input data of grayscale
        weights_conv1 = self.pretrained_model.conv1.weight
        weights_conv1 = torch.mean(weights_conv1, dim=1, keepdim=True)
        self.pretrained_model.conv1 = nn.Conv2d(1, self.pretrained_model.conv1.out_channels, kernel_size=7, stride=2, padding=3, bias=False)
        self.pretrained_model.conv1.weight.data = weights_conv1.data

        # modify last layer to fit output data of 10 classes, remove pretrained weights for last layers
        self.pretrained_model.fc = nn.Linear(self.pretrained_model.fc.in_features, numberOfClasses)

    def forward(self,x):
        return self.pretrained_model.forward(x)

class CNN(nn.Module):
    def __init__(self,numberOfClasses):
        super(CNN,self).__init__()
        self.cnn1=nn.Conv2d(in_channels=1,out_channels=2,kernel_size=3,padding=1)
        self.r1=nn.ReLU();
        self.cnn2 = nn.Conv2d(in_channels=2, out_channels=4,kernel_size=3,padding=1)
        self.r2 = nn.ReLU();
        self.cnn3 = nn.Conv2d(in_channels=4, out_channels=4,kernel_size=3,padding=1)
        self.r3 = nn.ReLU();
        self.cnn4 = nn.Conv2d(in_channels=4, out_channels=4,kernel_size=3,padding=1)
        self.r4 = nn.ReLU();
        self.fc=nn.Linear(in_features=28*28*4,out_features=numberOfClasses)
    def forward(self,x):
        out=self.cnn1(x);
        out=self.r1(out)
        out=self.cnn2(out);
        z1=self.r2(out)
        out=self.cnn3(z1+x);
        out=self.r3(out)
        out=self.cnn4(out+z1);
        z2=self.r4(out)
        out=out.reshape(batchSize,28*28*4)
        out=self.fc(out)
        return out
net=CNN(10)
lr=0.003
loss=nn.CrossEntropyLoss()
optimizer=torch.optim.Adam(net.parameters(),lr=lr)

for i in range(10):

    #TODO implement mini batches
    #TODO implement train and test data loader phases
    for j,(x,y) in enumerate(trainSet):
        #TODO add mini batches (hack fix just expand dims)
        x.to(device)
        y.to(device)
        ypred=net(x)
        ypred=ypred.to(torch.float32)


        l = loss(ypred,y)
        l.backward()
        optimizer.step()
        optimizer.zero_grad()
        if j%100==0:
            print("Iteration:",i,"batch:",j,"Loss:",l.item())
    correct = 0
    with torch.no_grad():
        for i in range(1):
            for j, (x, y) in enumerate(trainSet):
                x.to(device)
                y.to(device)
                ypred = net(x)
                temp = torch.argmax(ypred, dim=1)
                correct += ((temp - y) == 0).sum()
    trainAcc=correct/len(trainSetX)

    corret=0
    with torch.no_grad():
        for i in range(1):
            for j,(x,y) in enumerate(testSet):
                x.to(device)
                y.to(device)
                ypred=net(x)
                temp=torch.argmax(ypred,dim=1)
                corret+=((temp-y)==0).sum()

    testAcc=corret/len(testX)

#TODO add to CSV
#TODO log metrics at every epoch: train_loss, test_loss, train_acc, test_acc, train_f1, test_f1
    print(f'training accuracy={trainAcc:.4f},test accuracy={testAcc:.4f}')
