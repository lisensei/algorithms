'''
cleanSet:50000 images without noise;
trainset:40000 clean images
noisySet:50000 images with certain percentage of images with noise
trainLoss:grand train loss of each iteration
testLoss:grand test loss of each iteration
'''
import torch
import torch.nn as nn
import torch.utils
import torch.utils.data as Data
import torch.nn.utils
import torchvision.transforms as transform
import torchvision.datasets
import torchvision.models
import sklearn.datasets as datasets
import matplotlib.pyplot as plt
import math
import time
import csv
import argparse

parser=argparse.ArgumentParser(description="Learning rate,noise percent and batch size")
parser.add_argument('Hyperparameters',metavar='F',type=float,nargs=3,help="Hyper parameters for training")
hp=parser.parse_args()

#TODO use argparse for passing hyper parameters (later we will use console based HPC instead of Google Colab)
#TODO use cuda for model training
def addPeperNoise(source,percent):
    source = source.squeeze(dim=1)
    for image in source:

        for j in range(int(255*percent/100)):
            x = torch.randint(0, 28, (1,))
            y = torch.randint(0, 28, (1,))
            if (x + y) % 2 == 0:
                image[x, y] = 0
            else:
                image[x, y] = 255

    return source.unsqueeze(dim=1)


def addGaussianNoise(source,std):
    source=source.squeeze(dim=1)
    for image in source:
        plt.imshow(image)
        plt.show()
        image=image/255
        image=image+torch.randn(image.size())*std
        image=torch.clamp(image,min=0,max=1)
        image=image*255
        plt.imshow(image)
        plt.show()
    return source.unsqueeze(dim=1)

device= torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
data=torchvision.datasets.MNIST(root='/data',transform=transform.ToTensor(),download=True)

lr=hp.Hyperparameters[0]
noisyPercent=hp.Hyperparameters[1]
batchSize=int(hp.Hyperparameters[2])

print("Learning rate set to",lr,"noisyPercent set to",noisyPercent,"batch size set to",batchSize)
X=data.data
X=X.to(torch.float32)
X=X.unsqueeze(dim=1)
Y=data.targets
sampleAmount=int(len(X)*4/6)
noisyAmount=int(len(X)*1/6)
cleanSet=Data.TensorDataset(X[:sampleAmount+noisyAmount],Y[:sampleAmount+noisyAmount])
noisyX=X[sampleAmount:sampleAmount+noisyAmount]
noisyY=Y[sampleAmount:sampleAmount+noisyAmount]


trainSetX=X[:sampleAmount]
trainSetY=Y[:sampleAmount]

trainSet=Data.TensorDataset(trainSetX,trainSetY)
testSet=Data.TensorDataset(X[sampleAmount+noisyAmount:],Y[sampleAmount+noisyAmount:])

cleanSet=Data.DataLoader(dataset=cleanSet,batch_size=batchSize,shuffle=True)
trainSet=Data.DataLoader(dataset=trainSet,batch_size=batchSize,shuffle=True)
testSet=Data.DataLoader(dataset=testSet,batch_size=batchSize,shuffle=False)

'''Create Noisy Set'''
noisyX=addPeperNoise(noisyX,noisyPercent)
noisySetX=torch.cat((trainSetX,noisyX))
noisySetY=torch.cat((trainSetY,noisyY))
noisySet=Data.TensorDataset(noisySetX,noisySetY)
noisySet=Data.DataLoader(dataset=noisySet,batch_size=batchSize,shuffle=True)




#TODO Research part:
# 1. Add noise 20% of image with gaussian (20% of image as value 0.0) to 40% of dataset
# 2. Try to train whole dataset at once
# 3. Try to train clean dataset first and then after 10 epochs add 20% of noisy dataset

#TODO implement dataset via for train and test part
# torch.utils.data.Dataset
# torch.utils.data.DataLoader



#TODO test out different pre-tained models
class Network(nn.Module):
    def __init__(self,inputFeatures,numberOfClasses):
        super(Network, self).__init__()

        # loaded pretrained model
        self.pretrained_model = torchvision.models.resnet18(pretrained=True)

        # modify first layer to fit the input data of grayscale
        weights_conv1 = self.pretrained_model.conv1.weight
        weights_conv1 = torch.mean(weights_conv1, dim=1, keepdim=True)
        self.pretrained_model.conv1 = nn.Conv2d(1, self.pretrained_model.conv1.out_channels, kernel_size=7, stride=2, padding=3, bias=False)
        self.pretrained_model.conv1.weight.data = weights_conv1.data

        # modify last layer to fit output data of 10 classes, remove pretrained weights for last layers
        self.pretrained_model.fc = nn.Linear(self.pretrained_model.fc.in_features, numberOfClasses)

    def forward(self,x):
        return self.pretrained_model.forward(x)

class CNN(nn.Module):
    def __init__(self,numberOfClasses):
        super(CNN,self).__init__()
        self.cnn1=nn.Conv2d(in_channels=1,out_channels=2,kernel_size=3,padding=1)
        self.r1=nn.ReLU();
        self.cnn2 = nn.Conv2d(in_channels=2, out_channels=4,kernel_size=3,padding=1)
        self.r2 = nn.ReLU();
        self.cnn3 = nn.Conv2d(in_channels=4, out_channels=4,kernel_size=3,padding=1)
        self.r3 = nn.ReLU();
        self.cnn4 = nn.Conv2d(in_channels=4, out_channels=4,kernel_size=3,padding=1)
        self.r4 = nn.ReLU();
        self.fc=nn.Linear(in_features=28*28*4,out_features=numberOfClasses)
    def forward(self,x):
        out=self.cnn1(x);
        out=self.r1(out)
        out=self.cnn2(out);
        z1=self.r2(out)
        out=self.cnn3(z1+x);
        out=self.r3(out)
        out=self.cnn4(out+z1);
        out=self.r4(out)
        out=out.reshape(batchSize,28*28*4)
        out=self.fc(out)
        return out
net=CNN(10)
loss=nn.CrossEntropyLoss()
optimizer=torch.optim.Adam(net.parameters(),lr=lr)

'''Training without curriculum:'''
print("Noise percent:",noisyPercent,". Training without curriculum starts:")
for i in range(20):
    trainLoss=0
    testLoss=0
    #TODO implement mini batches
    #TODO implement train and test data loader phases
    trainCorrect=0

    for j,(x,y) in enumerate(noisySet):
        #TODO add mini batches (hack fix just expand dims)
        x.to(device)
        y.to(device)
        ypred=net(x)
        ypred=ypred.to(torch.float32)
        temp = torch.argmax(ypred, dim=1)
        trainCorrect += ((temp - y) == 0).sum()

        l = loss(ypred,y)
        trainLoss+=l

        l.backward()
        optimizer.step()
        optimizer.zero_grad()
        if j%1000==0:
            print("Iteration:",i,"batch:",j,"Loss:",l.item())
    trainAcc=trainCorrect/(sampleAmount+noisyAmount)

    testCorrect=0
    with torch.no_grad():
        for i in range(1):
            for j,(x,y) in enumerate(testSet):
                x.to(device)
                y.to(device)
                ypred=net(x)
                l=loss(ypred,y)
                testLoss+=l
                temp=torch.argmax(ypred,dim=1)
                testCorrect+=((temp-y)==0).sum()

    testAcc=testCorrect/(len(X)-sampleAmount-noisyAmount)
    import csv

    with open('metrics_without_curriculum.csv', 'a', newline='') as csvfile:
        fwrite = csv.writer(csvfile, delimiter=',')
        fwrite.writerow([noisyPercent,trainLoss.item(),testLoss.item(),trainAcc.item(),testAcc.item()])

#TODO add to CSV
#TODO log metrics at every epoch: train_loss, test_loss, train_acc, test_acc, train_f1, test_f1
    print(f'training accuracy={trainAcc:.4f},test accuracy={testAcc:.4f}')
'''Training with curriculum:'''
print("Noise percent:",noisyPercent,".Training with curriculum starts:")
for i in range(20):
    trainLoss=0
    testLoss=0
    #TODO implement mini batches
    #TODO implement train and test data loader phases
    trainCorrect = 0
    if i<10:
        for j,(x,y) in enumerate(trainSet):
            #TODO add mini batches (hack fix just expand dims)
            x.to(device)
            y.to(device)
            ypred=net(x)
            ypred=ypred.to(torch.float32)
            temp = torch.argmax(ypred, dim=1)
            trainCorrect += ((temp - y) == 0).sum()

            l = loss(ypred,y)
            trainLoss+=l
            l.backward()
            optimizer.step()
            optimizer.zero_grad()
            if j%1000==0:
                print("Iteration:",i,"batch:",j,"Loss:",l.item())

    else:
        for j, (x, y) in enumerate(noisySet):
            # TODO add mini batches (hack fix just expand dims)
            x.to(device)
            y.to(device)
            ypred = net(x)
            ypred = ypred.to(torch.float32)
            temp = torch.argmax(ypred, dim=1)
            trainCorrect += ((temp - y) == 0).sum()

            l = loss(ypred, y)
            trainLoss+=l
            l.backward()
            optimizer.step()
            optimizer.zero_grad()
            if j % 1000 == 0:
                print("Iteration:", i, "batch:", j, "Loss:", l.item())
    if(i<10):
        trainAcc = trainCorrect / sampleAmount
    else:
        trainAcc=trainCorrect/(sampleAmount+noisyAmount)

    testCorrect=0
    with torch.no_grad():
        for i in range(1):
            for j,(x,y) in enumerate(testSet):
                x.to(device)
                y.to(device)
                ypred=net(x)
                l=loss(ypred,y)
                testLoss+=l
                temp=torch.argmax(ypred,dim=1)
                testCorrect+=((temp-y)==0).sum()

    testAcc=testCorrect/(len(X)-sampleAmount-noisyAmount)
    import csv

    with open('metrics_with_curriculum.csv', 'a', newline='') as csvfile:
        fwrite = csv.writer(csvfile, delimiter=',')
        fwrite.writerow([noisyPercent,trainLoss.item(),testLoss.item(),trainAcc.item(),testAcc.item()])

#TODO add to CSV
#TODO log metrics at every epoch: train_loss, test_loss, train_acc, test_acc, train_f1, test_f1
    print(f'training accuracy={trainAcc:.4f},test accuracy={testAcc:.4f}')
